# Mirror of Apple's iOS samples

This repository mirrors [Apple's iOS samples](https://developer.apple.com/library/ios/navigation/#section=Resource%20Types&topic=Sample%20Code).

| Name | Topic | Framework | Description |
| --- | --- | --- | --- |
| [<sup>ABUIGroups</sup>](https://developer.apple.com/library/ios/samplecode/ABUIGroups/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Contact Data)</sup> | <sup>AddressBook</sup> | <sup>ABUIGroups shows how to check and request access to a user’s address book database. It also demonstrates how to retrieve, add, and remove group records using AddressBook APIs. It displays groups organized by their source in the address book.</sup> |
| [<sup>AccelerometerGraph</sup>](https://developer.apple.com/library/ios/samplecode/AccelerometerGraph/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Device Information)</sup> | <sup>UIKit</sup> | <sup>AccelerometerGraph sample application graphs the motion of the device. It demonstrates how to use the UIAccelerometer class and how to use Quartz2D and Core Animation to provide a high performance graph view. It also demonstrates a low-pass filter that you can use to isolate the effects of gravity, and a high-pass filter that you can use to remove the effects of gravity.</sup> |
| [<sup>AdaptivePhotos: An Adaptive Application</sup>](https://developer.apple.com/library/ios/samplecode/AdaptivePhotos/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>This sample shows how to use new APIs introduced in iOS 8 to make your application work great on all devices and orientations. It uses size classes, traits, and additions to view controllers to make an app that works great at any size and configuration.</sup> |
| [<sup>AddMusic</sup>](https://developer.apple.com/library/ios/samplecode/AddMusic/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>MediaPlayer</sup> | <sup>AddMusic demonstrates basic use of iPod library access, part of the Media Player framework. You use iPod library access to play songs, audio books, and audio podcasts that are synced from a user's desktop iTunes library. This sample uses the Media Player framework's built-in user interface for choosing music.<br/> <br/> AddMusic also demonstrates how to mix application audio with iPod library audio. The sample includes code for configuring application audio behavior using the AVAudioSession class and Audio Session Services.</sup> |
| [<sup>AdvancedURLConnections</sup>](https://developer.apple.com/library/ios/samplecode/AdvancedURLConnections/Introduction/Intro.html) | <sup>Networking & Internet</sup><br/><sup>(Protocol Streams)</sup> | <sup>Foundation</sup> | <sup>This sample demonstrates various advanced networking techniques with NSURLConnection.  Specifically, it demonstrates how to respond to authentication challenges, how to modify the default server trust evaluation (for example, to support a server with a self-signed certificate), and how to provide client identities.</sup> |
| [<sup>Adventure: Building a SpriteKit Game Using Swift</sup>](https://developer.apple.com/library/ios/samplecode/Adventure-Swift/Introduction/Intro.html) | <sup>Languages & Utilities</sup><br/><sup>(Swift)</sup> | <sup>SpriteKit</sup> | <sup>This sample shows how to build a simple 2D game for iOS and OS X using SpriteKit and Swift.</sup> |
| [<sup>AirDrop Examples</sup>](https://developer.apple.com/library/ios/samplecode/sc2273/Introduction/Intro.html) | <sup>Networking & Internet</sup> | <sup></sup> | <sup>"AirDropSample" demonstrates three use cases for incorporating AirDrop into an app.</sup> |
| [<sup>AirLocate: Using CoreLocation to monitor, range, and configure your device as an iBeacon</sup>](https://developer.apple.com/library/ios/samplecode/AirLocate/Introduction/Intro.html) | <sup></sup> | <sup>CoreLocation</sup> | <sup>"AirLocate" demonstrates CoreLocation fencing and ranging of iBeacons, BTLE devices enabled to aide iOS devices in determining a users proximity to a location rather than their position.  Obtaining a users proximity with iBeacons is ideal in more intimate locations such as indoors where other positioning methods either do not work, or do not give the desired level of accuracy an iBeacon can provide.  In addition to demonstrating how to use CoreLocation's CLLocationManager APIs to monitor and range for these CLBeaconRegions, AirLocate also provides an example of how to calibrate and configure an iOS device as a beacon.</sup> |
| [<sup>Alternate Views</sup>](https://developer.apple.com/library/ios/samplecode/AlternateViews/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Windows & Views)</sup> | <sup>UIKit</sup> | <sup>This sample demonstrates how to implement alternate or distinguishing views for particular device orientations.  Doing so can be useful if your app displays different content between orientations or if your app uses vastly different layouts between orientations which cannot be reconciled by auto layout or programatic layout alone.</sup> |
| [<sup>Application Icons and Launch Images for iOS</sup>](https://developer.apple.com/library/ios/samplecode/Icons/Introduction/Intro.html) | <sup>General</sup> | <sup>UIKit</sup> | <sup>Every app is required to include an app icon.  It is recommended that apps also provide icons for: Spotlight, the Settings app, and when creating an Ad Hoc build and adding it to iTunes.  See QA1686: App Icons on iPad and iPhone, for a complete listing of icons required for iPhone, iPad, and Universal apps <https://developer.apple.com/library/ios/qa/qa1686/_index.html>.</sup> |
| [<sup>AppPrefs: Storing and Retrieving User Preferences</sup>](https://developer.apple.com/library/ios/samplecode/AppPrefs/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Preference Settings)</sup> | <sup>UIKit</sup> | <sup>Demonstrates how to display your app's user configurable options (preferences) in the "Settings" system application.  A settings bundle, included in your application’s bundle directory, contains the information needed by the Settings application to display your preferences and make it possible for the user to modify them.  The Settings application saves any configured values in the defaults database so that your application can retrieve them at runtime.<br/><br/>This sample also shows how to launch the Settings app from your application and how to dynamically update your application's UI when its settings are changed while the app is in the background.</sup> |
| [<sup>AQOfflineRenderTest</sup>](https://developer.apple.com/library/ios/samplecode/AQOfflineRenderTest/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AudioToolbox</sup> | <sup>Demonstrates using Audio Queue offline render functionality and the  AudioQueueOfflineRender API. The sample produces LPCM output buffers from an ALAC encoded source which are then written to a .caf file. The output.caf file is then played back confirming the offline functionality worked as expected. All the code demonstrating the Audio Queue is in a single file called aqofflinerender.cpp.</sup> |
| [<sup>Audio Converter File Convert Test</sup>](https://developer.apple.com/library/ios/samplecode/iPhoneACFileConvertTest/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AudioToolbox</sup> | <sup>Demonstrates using the Audio Converter APIs to convert from a PCM audio format to a compressed format including AAC.</sup> |
| [<sup>Audio Mixer (MixerHost)</sup>](https://developer.apple.com/library/ios/samplecode/MixerHost/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AudioUnit</sup> | <sup>MixerHost demonstrates how to use the Multichannel Mixer audio unit in an iOS application. It also demonstrates how to use a render callback function to provide audio to an audio unit input bus. In this sample, the audio delivered by the callback comes from two short loops read from disk. You could use a similar callback, however, to synthesize sounds to feed into a mixer unit. This sample is described in Audio Unit Hosting Guide for iOS.</sup> |
| [<sup>Audio UI Sounds (SysSound)</sup>](https://developer.apple.com/library/ios/samplecode/SysSound/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AudioToolbox</sup> | <sup>Demonstrates use of System Sound Services (AudioToolbox/AudioServices.h) to play alerts and user-interface sound effects, and to invoke vibration.</sup> |
| [<sup>aurioTouch</sup>](https://developer.apple.com/library/ios/samplecode/aurioTouch/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AudioUnit</sup> | <sup>aurioTouch demonstrates use of the remote i/o audio unit for handling audio input and output. The application can display the input audio in one of the forms, a regular time domain waveform, a frequency domain waveform (computed by performing a fast fourier transform on the incoming signal), and a sonogram view (a view displaying the frequency content of a signal over time, with the color signaling relative power, the y axis being frequency and the x as time). Tap the sonogram button to switch to a sonogram view, tap anywhere on the screen to return to the oscilloscope. Tap the FFT button to perform and display the input data after an FFT transform. Pinch in the oscilloscope view to expand and contract the scale for the x axis.</sup> |
| [<sup>AVARLDelegateDemo</sup>](https://developer.apple.com/library/ios/samplecode/sc1791/Introduction/Intro.html) | <sup>Audio & Video</sup> | <sup>AVFoundation</sup> | <sup>The sample code depicts three different use cases of AVAssetResourceLoaderDelegate (for Identity encryption use case scenarios) for HLS (HTTP Live streaming): - Redirect handler (redirection for the HTTP live streaming media files) - Fetching Encryption keys for the HTTP live streaming media (segments) - Custom play list generation (index file) for the HTTP live streaming.</sup> |
| [<sup>AVCam for iOS</sup>](https://developer.apple.com/library/ios/samplecode/AVCam/Introduction/Intro.html) | <sup>Audio & Video</sup> | <sup>AVFoundation</sup> | <sup>AVCam demonstrates how to use the AV Foundation capture APIs for recording movies and taking still images. There is a record button for recording movies, a camera button for switching between front and back cameras (on supported devices), and a still button for taking still images. It runs only on an actual device, either an iPad or iPhone, and cannot be run in the simulator.</sup> |
| [<sup>AVCamManual: Using the Manual Capture API</sup>](https://developer.apple.com/library/ios/samplecode/AVCamManual/Introduction/Intro.html) | <sup></sup> | <sup>AVFoundation</sup> | <sup>AVCamManual adds manual controls for focus, exposure, and white balance to the AVCam sample application.</sup> |
| [<sup>AVCaptureAudioDataOutput To AudioUnit iOS</sup>](https://developer.apple.com/library/ios/samplecode/AVCaptureToAudioUnit/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AVFoundation</sup> | <sup>AVCaptureToAudioUnit for iOS demonstrates how to use the CMSampleBufferRefs vended by AVFoundation's capture AVCaptureAudioDataOutput object with various CoreAudio APIs. The application uses a AVCaptureSession with a AVCaptureAudioDataOutput to capture audio from the default input, applies an effect to that audio using a simple delay effect AudioUnit and writes the modified audio to a file using the CoreAudio ExtAudioFile API. It also demonstrates using and AUGraph containing an AUConverter to convert the AVCaptureAudioDataOutput provided data format into a suitable format for the delay effect.</sup> |
| [<sup>AVCompositionDebugVieweriOS</sup>](https://developer.apple.com/library/ios/samplecode/AVCompositionDebugVieweriOS/Introduction/Intro.html) | <sup>Audio & Video</sup> | <sup>AVFoundation</sup> | <sup>This sample application has an AVCompositionDebugView which presents a visual description of the underlying AVComposition, AVVideoComposition and AVAudioMix objects which form the composition made using two clips, adding a cross fade transition in between and audio ramps to the two audio tracks. The visualization provided by the sample can be used as a debugging tool to discover issues with an incorrect composition/video composition. For example: a break in video composition would render black frames to screen, which can easily be detected using the visualization in the sample.</sup> |
| [<sup>AVCustomEdit</sup>](https://developer.apple.com/library/ios/samplecode/AVCustomEdit/Introduction/Intro.html) | <sup>Audio & Video</sup> | <sup>AVFoundation</sup> | <sup>The sample demonstrates the use of custom compositors to add transitions to an AVMutableComposition. It implements the AVVideoCompositing and AVVideoCompositionInstruction protocols to have access to individual source frames, which are then be rendered using OpenGL off screen rendering.</sup> |
| [<sup>AVLoupe</sup>](https://developer.apple.com/library/ios/samplecode/AVLoupe/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Video)</sup> | <sup>AVFoundation</sup> | <sup>This sample demonstrates how to use multiple synchronized AVPlayerLayer instances, associated with a single AVPlayer, to efficiently produce non-trivial presentation of timed visual media. Using just one AVPlayer this sample demonstrates how you can display the same video in multiple AVPlayerLayers simultaneously. With minimal code you can create very customized and creative forms of video display. As an example, this sample demonstrates an interactive loupe, or magnifying glass, for video playback. This is similar to features that you might have used in iPhoto and Aperture.</sup> |
| [<sup>AVMovieExporter</sup>](https://developer.apple.com/library/ios/samplecode/AVMovieExporter/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Video)</sup> | <sup>AVFoundation</sup> | <sup>This universal sample application reads movie files from the Asset Library and Media Library then exports them to a new media file using user defined settings. The user can adjust the exported file in the following ways:</sup> |
| [<sup>AVPlayerDemo</sup>](https://developer.apple.com/library/ios/samplecode/AVPlayerDemo/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Video)</sup> | <sup>AVFoundation</sup> | <sup>Uses AVPlayer to play videos from the iPod Library, Camera Roll, or via iTunes File Sharing. Also displays metadata.</sup> |
| [<sup>AVSimpleEditoriOS</sup>](https://developer.apple.com/library/ios/samplecode/AVSimpleEditoriOS/Introduction/Intro.html) | <sup>Audio & Video</sup> | <sup>AVFoundation</sup> | <sup>AVSimpleEditor is a simple AVFoundation based movie editing application which exercises the APIs of AVVideoComposition, AVAudioMix and demonstrates how they can be used for simple video editing tasks. It also demonstrates how they interact with playback (AVPlayerItem) and export (AVAssetExportSession). The application performs trim, rotate, crop, add music, add watermark and export. This sample is ARC-enabled.</sup> |
| [<sup>AVTimedAnnotationWriter: Using Custom Annotation Metadata for Movie Writing and Playback</sup>](https://developer.apple.com/library/ios/samplecode/AVTimedAnnotationWriter/Introduction/Intro.html) | <sup></sup> | <sup>AVFoundation</sup> | <sup>Demonstrates how to use the AVAssetWriterInputMetadataAdaptor API to write circle annotation metadata during video playback. The captured movie file has video, audio and metadata track. The metadata track contains circle annotation which is vended during playback using AVPlayerItemMetadataOutput.</sup> |
| [<sup>avTouch</sup>](https://developer.apple.com/library/ios/samplecode/avTouch/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AVFoundation</sup> | <sup>The avTouch sample demonstrates use of the AVAudioPlayer class for basic audio playback.</sup> |
| [<sup>Bananas: A simple SceneKit platforming game</sup>](https://developer.apple.com/library/ios/samplecode/Bananas/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>SceneKit</sup> | <sup>This sample shows how to build a basic game using Scene Kit, demonstrating physics, rendering techniques, lighting, actions and animation.</sup> |
| [<sup>Blurring and Tinting an Image</sup>](https://developer.apple.com/library/ios/samplecode/UIImageEffects/Introduction/Intro.html) | <sup>Graphics & Animation</sup> | <sup>UIKit</sup> | <sup>UIImageEffects demonstrates how to create and apply blur and tint effects to an image using the vImage, Quartz, and UIKit frameworks. The vImage framework is suited for high-performance image processing. Using vImage, your app gets all the benefits of vector processing without the need for you to write vectorized code.</sup> |
| [<sup>BonjourWeb</sup>](https://developer.apple.com/library/ios/samplecode/BonjourWeb/Introduction/Intro.html) | <sup>Networking & Internet</sup><br/><sup>(Services & Discovery)</sup> | <sup>Foundation</sup> | <sup>This application illustrates the fundamentals of browsing for network services using Bonjour. The BonjourBrowser hierarchically displays Bonjour domains and services as table views in a navigation controller. The contents of the table views are discovered and updated dynamically using NSNetServiceBrowser objects. Tapping an item in the services table causes the corresponding NSNetService object to be resolved asynchronously.  When that resolution completes, a delegate method is called which constructs a URL and opens it in Safari.</sup> |
| [<sup>BracketStripes: Using the Bracketed Capture API</sup>](https://developer.apple.com/library/ios/samplecode/BracketStripes/Introduction/Intro.html) | <sup></sup> | <sup>AVFoundation</sup> | <sup>BracketStripes<br/><br/>This sample illustrates the use of still image bracketing APIs available in AVFoundation.<br/><br/>Two types of brackets are demonstrated:<br/><br/>1. Auto-exposure brackets with exposure target bias, and<br/><br/>2. Manual exposure with control over ISO and exposure duration.<br/><br/>As each of the bracketed frames are captured in real-time, they are "striped" into a destination image buffer and later shown in a modal image viewer so each of the captured frames can be compared side-by-side.</sup> |
| [<sup>Breadcrumb</sup>](https://developer.apple.com/library/ios/samplecode/Breadcrumb/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>MapKit</sup> | <sup>Demonstrates how to draw a path using the Map Kit overlay, MKOverlayView, that follows and tracks the user's current location. The included CrumbPath and CrumbPathView overlay and overlay view classes can be used for any path of points that are expected to change over time. It also demonstrates what is needed to track the user's location as a background process.</sup> |
| [<sup>BTLE Central Peripheral Transfer</sup>](https://developer.apple.com/library/ios/samplecode/BTLE_Transfer/Introduction/Intro.html) | <sup></sup> | <sup>CoreBluetooth</sup> | <sup>This sample shows how to transfer data from an iOS device in CoreBluetooth Peripheral Mode to another in Central Mode, by using a CBCharacteristic on the Peripheral side that changes its value.  The value change is automatically picked up on the Central side.</sup> |
| [<sup>Checking and Requesting Access to Data Classes in Privacy Settings</sup>](https://developer.apple.com/library/ios/samplecode/PrivacyPrompts/Introduction/Intro.html) | <sup>Security</sup> | <sup></sup> | <sup>"PrivacyPrompts" shows how to check and request access to data classes such as Location, Contacts, and social media in Privacy Settings on iOS.</sup> |
| [<sup>CloudCaptions: How integrate CloudKit into your application</sup>](https://developer.apple.com/library/ios/samplecode/CloudCaptions/Introduction/Intro.html) | <sup></sup> | <sup>CloudKit</sup> | <sup>This sample shows how to use CloudKit to upload and retrieve CKRecords and associated assets. In this example, there are two record types, an image record type and a post record type. Users are able to upload their own photos or select an image already found in an image record type. This example also uses an NSPredicate in its CKQueries to filter results based on tags.</sup> |
| [<sup>CloudKitAtlas: An Introduction to CloudKit</sup>](https://developer.apple.com/library/ios/samplecode/CloudAtlas/Introduction/Intro.html) | <sup></sup> | <sup>CloudKit</sup> | <sup>CloudKitAtlas is a sample intended as a quick introduction to CloudKit. It teaches you how to use discoverability to get the first name and last name of the user logged into iCloud. It can add a CKRecord with a location and query for CKRecords near a location. You can upload and retrieve images as CKAssets. It also shows how to use CKReferences with CKReferenceActionDeleteSelf so the child records are deleted when the parent record is deleted. Finally, it also shows how to use CKSubscription to get push notifications when a new item is added for a record type.</sup> |
| [<sup>code:Explained Adventure</sup>](https://developer.apple.com/library/ios/documentation/GraphicsAnimation/Conceptual/CodeExplainedAdventure/AdventureArchitecture/AdventureArchitecture.html) | <sup>Graphics & Animation</sup> | <sup>SpriteKit</sup> | <sup>SDK Requirements: Please note that the Adventure iOS target requires the iOS 7 SDK. The OS X target requires the OS X 10.9 SDK.</sup> |
| [<sup>Collection View Transition</sup>](https://developer.apple.com/library/ios/samplecode/CollectionViewTransition/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Windows & Views)</sup> | <sup>UIKit</sup> | <sup>This sample illustrates how to create a custom transition when navigating between two collection views in a navigation hierarchy managed by a navigation controller.  The transition can be interrupted and reversed.  It uses a subclass of UICollectionViewTransitionLayout to help in the transition of the cell positions based on gesture position.</sup> |
| [<sup>CollectionView-Simple</sup>](https://developer.apple.com/library/ios/samplecode/CollectionView-Simple/Introduction/Intro.html) | <sup></sup> | <sup>UIKit</sup> | <sup>Demonstrates how to use UICollectionView, a way to present ordered data to users in a grid-like fashion.  With a collection view object, you are able to define the presentation and arrangement of embedded views.  The collection view class works closely with an accompanying layout object to define the placement of individual data items.  In this example UIKit provides a standard flow-based layout object that you can use to implement multi-column grids containing items of a standard size.</sup> |
| [<sup>Concurrent operations using Core Data</sup>](https://developer.apple.com/library/ios/samplecode/ThreadedCoreData/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Tables)</sup> | <sup>CoreData</sup> | <sup>Demonstrates how to use Core Data in a multi-threaded environment, following the first recommended pattern mentioned in the Core Data Programming Guide.</sup> |
| [<sup>Core Audio Utility Classes</sup>](https://developer.apple.com/library/ios/samplecode/CoreAudioUtilityClasses/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>CoreAudio</sup> | <sup>The "CoreAudio" folder contains the Public Utility sources (PublicUtility folder) as well as base classes required for codec and audio unit development. These utility classes are used by various Apple Core Audio sample project and extend or wrap Core Audio API's.</sup> |
| [<sup>Core Data Transformable Attributes</sup>](https://developer.apple.com/library/ios/samplecode/PhotoLocations/Introduction/Intro.html) | <sup>Data Management</sup> | <sup>CoreData</sup> | <sup>This sample illustrates a Core Data application that uses more than one entity and uses transformable attributes. It also shows inferred migration of the persistent store.</sup> |
| [<sup>Core Image Filters with Photos and Video for iOS</sup>](https://developer.apple.com/library/ios/samplecode/CIFunHouse/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(2D Drawing)</sup> | <sup>CoreImage</sup> | <sup>The CIFunHouse project shows how to apply Core Image built in and custom CIFilters to photos and video.  The application presents view controllers for adding photo and video sources, choosing CIFilters from a list, and making live adjustments to filter parameters.  The project also contains code for custom CIFilter subclasses for effect such as Sobel edge detection, old-style-film, and fake-depth-of-field looks.  The code also demonstrates how to save a filtered video stream to the ALAssetsLibrary while simultaneously previewing the video on the display.</sup> |
| [<sup>CoreBluetooth Temperature Sensor</sup>](https://developer.apple.com/library/ios/samplecode/TemperatureSensor/Introduction/Intro.html) | <sup></sup> | <sup>CoreBluetooth</sup> | <sup>A simple iOS iPhone application that demonstrates how to use the CoreBluetooth Framework to connect to a Bluetooth LE peripheral and read, write and be notified of changes to the characteristics of the peripheral.</sup> |
| [<sup>CoreDataBooks</sup>](https://developer.apple.com/library/ios/samplecode/CoreDataBooks/Introduction/Intro.html) | <sup>Data Management</sup> | <sup>CoreData</sup> | <sup>This sample illustrates a number of aspects of working with the Core Data framework with an iOS application:</sup> |
| [<sup>CoreTextPageViewer</sup>](https://developer.apple.com/library/ios/samplecode/CoreTextPageViewer/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Windows & Views)</sup> | <sup>CoreText</sup> | <sup>This sample shows how to use Core Text to display large bodies of text, text with mixed styles, and text with special style or layout requirements, such as use of custom fonts.  A version of this sample was used in the "Advanced Text Handling for iPhone OS" WWDC 2010 Session.</sup> |
| [<sup>CryptoExercise</sup>](https://developer.apple.com/library/ios/samplecode/CryptoExercise/Introduction/Intro.html) | <sup>Security</sup> | <sup>Security</sup> | <sup>This sample demonstrates the use of the two main Cryptographic API sets on the iPhone OS SDK. Asymmetric Key Encryption and random nonce generation is handled through the Security framework API set, whereas, Symmetric Key Encryption and Digest generation is handled by the CommonCrypto API set. The CryptoExercise sample brings both of these APIs together through a network service, discoverable via Bonjour, that performs a "dummy" cryptographic protocol between devices found on the same subnet.</sup> |
| [<sup>CurrentAddress</sup>](https://developer.apple.com/library/ios/samplecode/CurrentAddress/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>MapKit</sup> | <sup>Demonstrates basic use of MapKit, displaying a map view and setting its region to current location.</sup> |
| [<sup>Custom Animatable Property</sup>](https://developer.apple.com/library/ios/samplecode/sc2284/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(Animation)</sup> | <sup>CoreGraphics</sup> | <sup>Shows how to leverage Core Animation’s timing and rendering callbacks to implement custom animatable properties for CALayer subclasses. This technique is supported whether your CALayer subclass belongs to a UIView or is standalone. Both explicit and implicit animation triggers are demonstrated, as well as basic and keyframe animation types.</sup> |
| [<sup>Custom Section Titles with NSFetchedResultsController</sup>](https://developer.apple.com/library/ios/samplecode/DateSectionTitles/Introduction/Intro.html) | <sup>Data Management</sup> | <sup>CoreData</sup> | <sup>"DateSectionTitles" shows how to create section information for NSFetchedResultsController using dates.</sup> |
| [<sup>CustomContentAccessibility</sup>](https://developer.apple.com/library/ios/samplecode/sc2216/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>This sample, previously known as WWDCMaps, shows you how to support accessibility in a custom drawing UIView and UIControl, demonstrates how to create an accessibility element for each map item, and implement UIAccessibilityContainer protocol in the container view to interact with iOS accessibility system. The Guided Access Restriction API, which is newly introduced in iOS 7 for restricting functions when Guided Access enabled, is also demonstrated in this sample.</sup> |
| [<sup>CustomHTTPProtocol</sup>](https://developer.apple.com/library/ios/samplecode/CustomHTTPProtocol/Introduction/Intro.html) | <sup>Networking & Internet</sup> | <sup>Foundation</sup> | <sup>CustomHTTPProtocol shows how to use an NSURLProtocol subclass to intercept the NSURLConnections made by a high-level subsystem that does not otherwise expose its network connections.  In this specific case, it intercepts the HTTPS requests made by a web view and overrides server trust evaluation, allowing you to browse a site whose certificate is not trusted by default.</sup> |
| [<sup>Customizing UINavigationBar</sup>](https://developer.apple.com/library/ios/samplecode/NavBar/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Controls)</sup> | <sup>UIKit</sup> | <sup>Demonstrates how to use UINavigationController and UIViewController classes together as building blocks to your application's user interface.  Use it as a reference when starting the development of your new application.  The various pages in this sample exhibit different ways of how to modify the navigation bar directly, using the appearance proxy, and by modifying the view controller's UINavigationItem.  Among the levels of customization are varying appearance styles, and applying custom left and right buttons known as UIBarButtonItems.</sup> |
| [<sup>DateCell</sup>](https://developer.apple.com/library/ios/samplecode/DateCell/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Tables)</sup> | <sup>UIKit</sup> | <sup>Demonstrates formatted display of date objects in table cells and use of UIDatePicker to edit those values.</sup> |
| [<sup>DocInteraction</sup>](https://developer.apple.com/library/ios/samplecode/DocInteraction/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(File Management)</sup> | <sup>UIKit</sup> | <sup>Demonstrates how to use UIDocumentInteractionController to obtain information about documents and how to preview them.  There are two ways to preview documents: one is to use UIDocumentInteractionController's preview API, the other is directly use QLPreviewController.  This sample also demonstrates the use of UIFileSharingEnabled feature so you can upload documents to the application using iTunes and then to preview them.  With the help of "kqueue" kernel event notifications, the sample monitors the contents of the Documents folder.</sup> |
| [<sup>DownloadFont</sup>](https://developer.apple.com/library/ios/samplecode/DownloadFont/Introduction/Intro.html) | <sup></sup> | <sup>CoreText</sup> | <sup>Demonstrates how to download fonts on demand on iOS 6 and later.</sup> |
| [<sup>EADemo</sup>](https://developer.apple.com/library/ios/samplecode/EADemo/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Device Information)</sup> | <sup>ExternalAccessory</sup> | <sup>The sample can be used with any Made For iPod (MFI) device designed for use with the External Accessory Framework. The application will display an External Accessory attached device in the Accessories window, provide information registered by the MFI device, and provides methods to send and receive data to the device.</sup> |
| [<sup>Enumeration Sample</sup>](https://developer.apple.com/library/ios/samplecode/FastEnumerationSample/Introduction/Intro.html) | <sup></sup> | <sup>Foundation</sup> | <sup>EnumerationSample is a command line project that demonstrates how to implement a class that supports block-based enumeration, fast enumeration, enumeration using NSEnumerator, and subscripting.  While provided as a OS X application, the techniques demonstrated by this sample are fully applicable to iOS development.</sup> |
| [<sup>Example app using Photos framework</sup>](https://developer.apple.com/library/ios/samplecode/UsingPhotosFramework/Introduction/Intro.html) | <sup></sup> | <sup>Photos</sup> | <sup>A basic Photos-like app which introduces the Photos framework. <br/><br/>- List albums, folders and moments<br/><br/>- Display the contents of the moments, or a single album<br/><br/>- Display the content of a single photo or video (and allow playback in the case of a video)<br/><br/>- Allow the following actions:<br/><br/>    * simple single-click edit of a photo<br/><br/>    * creating an album and adding assets to it<br/><br/>    * re-ordering of assets in an album<br/><br/>    * removing assets from an album<br/><br/>    * deleting assets and albums<br/><br/>    * (un)hiding an asset from moments<br/><br/>    * favoriting an asset</sup> |
| [<sup>Extended Audio File Conversion Test</sup>](https://developer.apple.com/library/ios/samplecode/iPhoneExtAudioFileConvertTest/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AudioToolbox</sup> | <sup>Demonstrates using ExtAudioFile API to convert from one audio format and file type to another.</sup> |
| [<sup>Fit: Store and Retrieve HealthKit Data</sup>](https://developer.apple.com/library/ios/samplecode/Fit/Introduction/Intro.html) | <sup></sup> | <sup>HealthKit</sup> | <sup>Fit is a sample intended as a quick introduction to HealthKit. It teaches you everything from writing data into HealthKit to reading data from HealthKit. This information may have been entered into the store by some other app; e.g. a user's birthday may have been entered into Health, and a user's weight by some popular weight tracker app. Fit shows examples of using queries to retrieve information from HealthKit using sample queries and statistics queries. Fit gives you a quick introduction into using the new Foundation classes NSLengthFormatter, NSMassFormatter, and NSEnergyFormatter.</sup> |
| [<sup>Footprint: Indoor Positioning with Core Location</sup>](https://developer.apple.com/library/ios/samplecode/footprint/Introduction/Intro.html) | <sup></sup> | <sup>CoreLocation</sup> | <sup>Use Core Location to take a Latitude/Longitude position and project it onto a flat floorplan. Demonstrates how to do the conversion from the Geographic coordinates system (Latitude/Longitude) to the floorplan's image coordinate system (x,y).</sup> |
| [<sup>GenericKeychain</sup>](https://developer.apple.com/library/ios/samplecode/GenericKeychain/Introduction/Intro.html) | <sup>Security</sup> | <sup>Security</sup> | <sup>This sample shows how to add, query for, remove, and update a keychain item of generic class type. Also demonstrates the use of shared keychain items. All classes exhibit very similar behavior so the included examples will scale to the other classes of Keychain Item: Internet Password, Certificate, Key, and Identity.</sup> |
| [<sup>GeocoderDemo</sup>](https://developer.apple.com/library/ios/samplecode/GeocoderDemo/Introduction/Intro.html) | <sup>Data Management</sup> | <sup>CoreLocation</sup> | <sup>This sample application demonstrates using a CLGeocoder instance to perform forward and reverse geocoding on strings and dictionaries. The application also includes an example distance calculator that will display the distance between two placemarks.</sup> |
| [<sup>Get Battery Status</sup>](https://developer.apple.com/library/ios/samplecode/BatteryStatus/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Device Information)</sup> | <sup>UIKit</sup> | <sup>Demonstrates the use of the battery status properties and notifications provided via the iOS SDK.</sup> |
| [<sup>GKAchievements</sup>](https://developer.apple.com/library/ios/samplecode/GKAchievements/Introduction/Intro.html) | <sup>General</sup> | <sup>GameKit</sup> | <sup>Abstract: Provide an example of how to successfully submit achievements and store them when submission fails.</sup> |
| [<sup>GKAuthentication</sup>](https://developer.apple.com/library/ios/samplecode/GKAuthentication/Introduction/Intro.html) | <sup>General</sup> | <sup>GameKit</sup> | <sup>An example of how to successfully authenticate using GameKit.</sup> |
| [<sup>GKLeaderboards</sup>](https://developer.apple.com/library/ios/samplecode/GKLeaderboards/Introduction/Intro.html) | <sup>General</sup> | <sup>GameKit</sup> | <sup>GKLeaderboard is a sample application that shows how to correctly submit a score and view them using GKLeaderboardViewController.</sup> |
| [<sup>GKTapper</sup>](https://developer.apple.com/library/ios/samplecode/GKTapper/Introduction/Intro.html) | <sup></sup> | <sup>GameKit</sup> | <sup>GKTapper is a sample application that shows how to support GameCenter Leaderboards and Achievements.  It also demonstrates using GKLeaderboardViewController and GKAchievementViewController to display this data.</sup> |
| [<sup>GLAirplay</sup>](https://developer.apple.com/library/ios/samplecode/GLAirplay/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>OpenGLES</sup> | <sup>Demonstrates how to provide a richer experience to your users when they are using Airplay by displaying your UI on the iPhone/iPad and your app/game contents on the second display.</sup> |
| [<sup>GLCameraRipple</sup>](https://developer.apple.com/library/ios/samplecode/GLCameraRipple/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Video)</sup> | <sup>AVFoundation</sup> | <sup>This sample demonstrates how to use the AVFoundation framework to capture YUV frames from the camera and process them using shaders in OpenGL ES 2.0. CVOpenGLESTextureCache, which is new to iOS 5.0, is used to provide optimal performance when using the AVCaptureOutput as an OpenGL texture. In addition, a ripple effect is applied by modifying the texture coordinates of a densely tessellated quad.</sup> |
| [<sup>GLEssentials</sup>](https://developer.apple.com/library/ios/samplecode/GLEssentials/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>OpenGLES</sup> | <sup>This sample provides examples of some essential techniques for using the OpenGL and OpenGL ES API.  It includes usages of Vertex Buffer Objects (VBOs), Vertex Array Objects (VAOs),  Framebuffer Objects (FBO), and GLSL Program Objects.  It creates a VAO and VBOs from model data loaded in.  It then creates a texture for the model from image data and GLSL shaders from source also loaded in.   Finally, it creates an FBO and texture to render a reflection of the model.  It uses an environment mapping GLSL program to apply the reflection texture to a plane.</sup> |
| [<sup>GLGravity</sup>](https://developer.apple.com/library/ios/samplecode/GLGravity/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>OpenGLES</sup> | <sup>The GLGravity sample application demonstrates how to use the UIAccelerometer class in combination with OpenGL rendering. It shows how to extract the gravity vector from the accelerometer values using a basic low-pass filter, and how to build an OpenGL transformation matrix from it.</sup> |
| [<sup>GLImageProcessing</sup>](https://developer.apple.com/library/ios/samplecode/GLImageProcessing/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>OpenGLES</sup> | <sup>The GLImageProcessing sample application demonstrates how to implement simple image processing filters (Brightness, Contrast, Saturation, Hue rotation, Sharpness) using OpenGL ES1.1. The sample also shows how to create simple procedural button icons using CoreGraphics.</sup> |
| [<sup>GLPaint</sup>](https://developer.apple.com/library/ios/samplecode/GLPaint/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>OpenGLES</sup> | <sup>The GLPaint sample application demonstrates how to support single finger painting using OpenGL ES. This sample also shows how to detect a "shake" motion of the device. By looking at the code you'll see how to set up an OpenGL ES view and use it for rendering painting strokes. The application creates a brush texture from an image by first drawing the image into a Core Graphics bitmap context. It then uses the bitmap data for the texture.</sup> |
| [<sup>GLTextureAtlas</sup>](https://developer.apple.com/library/ios/samplecode/GLTextureAtlas/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>OpenGLES</sup> | <sup>This sample demonstrates how to use a texture atlas to draw multiple objects with different textures simultaneously using OpenGL ES. The application uses a texture atlas in the PVR format. By adding in degenerated triangles, and compute 3D transformations ourselves using matrices, we are able to collapse all the draw calls into one.</sup> |
| [<sup>Handling Touches Using Responder Methods and Gesture Recognizers</sup>](https://developer.apple.com/library/ios/samplecode/Touches/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Event Handling)</sup> | <sup>UIKit</sup> | <sup>This sample contains two applications that demonstrate how to handle touches, including multiple touches that move multiple objects: "Touches_Responder" demonstrates how to handle touches using UIResponder's: touches began, touches moved, and touches ended methods. "Touches_GestureRecognizers" demonstrates how to use UIGestureRecognizer objects to handle touch events.</sup> |
| [<sup>HazardMap</sup>](https://developer.apple.com/library/ios/samplecode/HazardMap/Introduction/Intro.html) | <sup></sup> | <sup>MapKit</sup> | <sup>Demonstrates how to create a custom Map Kit overlay to display USGS earthquake hazard data.  It shows how to create a custom Map Kit overlay and corresponding view to display USGS earthquake hazard data on top of an MKMapView.</sup> |
| [<sup>HeadsUpUI</sup>](https://developer.apple.com/library/ios/samplecode/HeadsUpUI/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Windows & Views)</sup> | <sup>UIKit</sup> | <sup>Demonstrates how to implement a Heads Up or HUD-like user interface over the app's primary view controller.  This essentially mimics the behavior of the MPMoviePlayerController's hovering controls for controlling movie playback.  Developers can refer to this sample for best practices in how to implement this translucent kind of interface complete with animation and timer support.</sup> |
| [<sup>HelloGoodbye: Using the Accessibility API to Widen Your User Base</sup>](https://developer.apple.com/library/ios/samplecode/HelloGoodbye/Introduction/Intro.html) | <sup></sup> | <sup>UIKit</sup> | <sup>This project shows you how to use the Accessibility API to widen your user base. It demonstrates how you can adjust your user interface when a user has Bold Text, Reduce Transparency, Darken Colors, or Reduce Motion enabled. It also contains examples of API you can implement to allow a VoiceOver or Switch Control user to interact with your app.</sup> |
| [<sup>HelloWorld</sup>](https://developer.apple.com/library/ios/samplecode/HelloWorld_iPhone/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>HelloWorld demonstrates how to use a keyboard to enter text into a text field and how to display the text in a label.</sup> |
| [<sup>HomeKit Catalog</sup>](https://developer.apple.com/library/ios/samplecode/HomeKitCatalog/Introduction/Intro.html) | <sup>General</sup> | <sup>HomeKit</sup> | <sup>Demonstrates how to use the HomeKit API, and provides a sample UI for creating homes, controlling accessories, grouping accessories into rooms and zones, creating action sets to tie together multiple actions, creating timer triggers to fire action sets at specific times, and creating service groups to group services into contexts.</sup> |
| [<sup>iAdInterstitialSuite</sup>](https://developer.apple.com/library/ios/samplecode/iAdInterstitialSuite/Introduction/Intro.html) | <sup></sup> | <sup>iAd</sup> | <sup>iAdInterstitialSuite contains two applications that demonstrate the usage of the ADInterstitialAd introduced in iOS 4.3.</sup> |
| [<sup>iAdSuite</sup>](https://developer.apple.com/library/ios/samplecode/iAdSuite/Introduction/Intro.html) | <sup></sup> | <sup>iAd</sup> | <sup>iAdSuite is a set of samples demonstrating how to manage an ADBannerView in many common scenarios, each scenario demonstrated in a particular sample application.</sup> |
| [<sup>iAdSuite with Storyboards</sup>](https://developer.apple.com/library/ios/samplecode/iAdSuite_Storyboard/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>iAd</sup> | <sup>iAdSuite is a set of samples demonstrating how to manage an ADBannerView in many common scenarios, each scenario demonstrated in a particular sample application.</sup> |
| [<sup>Inter-App Audio Examples</sup>](https://developer.apple.com/library/ios/samplecode/InterAppAudioSuite/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AudioUnit</sup> | <sup>This suite of samples includes three projects that together illustrate Inter-App Audio feature.</sup> |
| [<sup>Internationalization and Localization for iOS</sup>](https://developer.apple.com/library/ios/samplecode/InternationalMountains/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Strings, Text, & Fonts)</sup> | <sup>UIKit</sup> | <sup>Drawing from the existing Cocoa Internationalization Mountains sample, this sample shows how to integrate, design and programmatically access localized resources and data in an iOS application. This sample uses multiple localized views, localized formatted strings, localized application data, localized info.plist strings, and a localized application preferences settings bundle. The sample is localized in three languages: English, French, and Traditional Chinese.</sup> |
| [<sup>iPhoneCoreDataRecipes</sup>](https://developer.apple.com/library/ios/samplecode/iPhoneCoreDataRecipes/Introduction/Intro.html) | <sup>Data Management</sup> | <sup>CoreData</sup> | <sup>This sample shows how you can use view controllers, table views, and Core Data in an iPhone application.</sup> |
| [<sup>iPhoneMultichannelMixerTest</sup>](https://developer.apple.com/library/ios/samplecode/iPhoneMultichannelMixerTest/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AudioUnit</sup> | <sup>Demonstrates how to build an Audio Unit Graph connecting a Multichannel Mixer instance to the RemoteIO unit. Two input busses are created each with input volume controls. An overall mixer output volume control is also provided and each bus may be enabled or disabled.</sup> |
| [<sup>KeyboardAccessory</sup>](https://developer.apple.com/library/ios/samplecode/KeyboardAccessory/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>Shows how to use a keyboard accessory view.</sup> |
| [<sup>KeychainTouchID: Using Touch ID with Keychain and LocalAuthentication</sup>](https://developer.apple.com/library/ios/samplecode/KeychainTouchID/Introduction/Intro.html) | <sup></sup> | <sup>LocalAuthentication</sup> | <sup>KeychainTouchID shows how to store Touch ID protected items to the keychain and how to query for the items with custom message prompts. It also shows how to use the new keychain item accessibility class which invalidates items when the passcode is removed. It also shows how to use LocalAuthentication to invoke Touch ID verification without involving the keychain.</sup> |
| [<sup>KMLViewer</sup>](https://developer.apple.com/library/ios/samplecode/KMLViewer/Introduction/Intro.html) | <sup></sup> | <sup>MapKit</sup> | <sup>Demonstrates how to display KML files on top of a MKMapView.  It shows how to use MapKit's Annotations and Overlays to display KML files on top of an MKMapView.</sup> |
| [<sup>Large Image Downsizing</sup>](https://developer.apple.com/library/ios/samplecode/LargeImageDownsizing/Introduction/Intro.html) | <sup>Graphics & Animation</sup> | <sup>CoreGraphics</sup> | <sup>This code sample demonstrates a way to support displaying very large images in limited memory environments by turning a large image on disk into a smaller image in memory. This is useful in situations where the original image is too large to fit into memory as required for it to be displayed.</sup> |
| [<sup>LaunchMe</sup>](https://developer.apple.com/library/ios/samplecode/LaunchMe/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Event Handling)</sup> | <sup>UIKit</sup> | <sup>The LaunchMe sample application demonstrates how to implement a custom URL scheme to allow other applications to interact with your application.  It registers the "launchme" URL scheme, of which URLs contain an HTML color code (e.g. #FF0000 or #F00).  The sample shows how to handle an incoming URL request by overriding -application:openURL:sourceApplication:annotation: to properly parse and extract information from the requested URL before updating the user interface.</sup> |
| [<sup>LazyTableImages</sup>](https://developer.apple.com/library/ios/samplecode/LazyTableImages/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Tables)</sup> | <sup>UIKit</sup> | <sup>This sample demonstrates a multi-stage approach to loading and displaying a UITableView.  It begins by loading the relevant text from an RSS feed so the table can load as quickly as possible, and then downloads the images for each row asynchronously so the UI is more responsive.</sup> |
| [<sup>ListAdder</sup>](https://developer.apple.com/library/ios/samplecode/ListAdder/Introduction/Intro.html) | <sup>Data Management</sup> | <sup>Foundation</sup> | <sup>This sample demonstrates the technique of thread confinement using NSOperation.  It was written to support TN2109 "Simple and Reliable Threading with NSOperation".</sup> |
| [<sup>Lister: A Productivity App (Obj-C and Swift)</sup>](https://developer.apple.com/library/ios/samplecode/Lister/Introduction/Intro.html) | <sup>User Experience</sup> | <sup></sup> | <sup>Lister is a list management app written in Swift that's built on top of iCloud and the powerful NSDocument and UIDocument architectures. In a single project you'll find both iOS and OS X targets, embedded frameworks, UI extensions, live rendering of custom views in Interface Builder, iOS and OS X Storyboards, Auto Layout, and more.<br/><br/>To abstract the storage mechanism away from the type of storage (iCloud or local), Lister uses a {AAPL}ListController class that notifies the {AAPL}ListDocumentsViewController about new lists, lists that have been removed, and also lists that have been updated. The {AAPL}ListController has an {AAPL}ListCoordinator property which is resonsible for tracking the relevant URLs. In Lister, there are two types of {AAPL}ListCoordinator objects: the {AAPL}CloudListCoordinator object as well as the {AAPL}LocalListCoordinator object. The only place that these objects are used directly is within the {AAPL}ListController. The storage mechanism is determined by the {AAPL}AppDelegate, which asks the user what their storage preference is. Once their preference is known, the app delegate creates an {AAPL}ListCoordinator and passes it to the app delegate's {AAPL}ListController property. The app delegate passes the {AAPL}ListController object throughout the application to ensure that it's used as the single place to manage lists.</sup> |
| [<sup>LocateMe</sup>](https://developer.apple.com/library/ios/samplecode/LocateMe/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Device Information)</sup> | <sup>CoreLocation</sup> | <sup>This demonstrates the two primary use cases for the Core Location Framework: getting the user's location and tracking changes to the user's location.</sup> |
| [<sup>LookInside: Presentation Controllers, Adaptivity, and Custom Animator Objects</sup>](https://developer.apple.com/library/ios/samplecode/LookInside/Introduction/Intro.html) | <sup></sup> | <sup>UIKit</sup> | <sup>This example shows how to use a custom presentation controller to create a custom view controller presentation. It provides a transitioning delegate to the view controller, which vends a presentation controller and animator object.</sup> |
| [<sup>Managed App Configuration</sup>](https://developer.apple.com/library/ios/samplecode/sc2279/Introduction/Intro.html) | <sup>General</sup> | <sup></sup> | <sup>"ManagedAppConfig" demonstrates how to implement managed app configuration and feedback support in an iOS application. This functionality allows a Mobile Device Management (MDM) server to push down a dictionary into the managed app's NSUserDefaults for the purposes of remotely configuring settings. Also, feedback (such as critical errors) can be written by the app into NSUserDefaults which can then be queried by an MDM server. This is a powerful mechanism enterprise and educational institutions can use to remotely configure managed applications from a centralized MDM server.</sup> |
| [<sup>MapCallouts</sup>](https://developer.apple.com/library/ios/samplecode/MapCallouts/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>MapKit</sup> | <sup>Demonstrates the use of the MapKit framework, displaying a map view with custom MKAnnotations each with custom callouts or custom MKAnnotationViews.  An annotation object on a map is any object that conforms to the MKAnnotation protocol and is displayed on the screen as a MKAnnotationView.  Through the use of the MKAnnotation protocol and MKAnnotationView, this application shows how you can extend annotations with custom strings and left/right calloutAccessoryViews.</sup> |
| [<sup>MapSearch</sup>](https://developer.apple.com/library/ios/samplecode/MapSearch/Introduction/Intro.html) | <sup>Data Management</sup> | <sup>MapKit</sup> | <sup>Demonstrates how to programmatically search for map-based addresses and points of interest using the MKLocalSearch class.  It initiates a search for map-based content using a natural language string.  A user can type "coffee", press search and it will find all the coffee places nearby.  The places found are centered around the user's current location. Once the search results have been found, the sample shows various ways to display the results.  It demonstrates how to use MKLocalSearchCompletionHandler and populate the UITableView with the search results.</sup> |
| [<sup>MessageComposer</sup>](https://developer.apple.com/library/ios/samplecode/MessageComposer/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>MessageUI</sup> | <sup>This application shows how to use the MessageUI framework to compose and send email and SMS messages from within your application.</sup> |
| [<sup>MetalBasic3D</sup>](https://developer.apple.com/library/ios/samplecode/MetalBasic3D/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>Metal</sup> | <sup>This Metal example has been ported from the Xcode OpenGL ES template and includes a reusable view class and a geometric math library built on top of the simd.h library. The math library has been created for performing geometric graphics operations within Metal's left hand coordinate system and is used in this sample to set up a basic perspective, look at and model view projection matrix for objects rendered in the scene. Each cube is renderered individually using a basic 3D lighting shader with diffuse and ambient components. Additional effects have been added to one of the spinning cubes from the template to demonstrate altering a single object's uniform values per frame while keeping the other constant, all the while sharing the same buffer in memory. The cube geometry is defined with interlaced vertex and normal data and demonstrates how stride can be read within a shader.</sup> |
| [<sup>MetalDeferredLighting</sup>](https://developer.apple.com/library/ios/samplecode/MetalDeferredLighting/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>Metal</sup> | <sup>MetalDeferredLighting is designed as an example of rendering of a g-buffer and light accumulation in a single render encoder in one pass using programmable blending. In this sample, we render in 2 passes. As such it is also an example of a multipass renderer in Metal. A "pass" in this case is defined as all draws to a texture before swapping it out for a new texture. <br/><br/>The first pass renders a shadow map based on the calculated position of a sun. The second pass performs a deferred lighting algorithm by writing to and reading from a framebuffer containing 4 color attachments.  Three of the attachments are seeded with the g-buffer values (albedo, linear depth, normal).  Next, light primitives are rendered to accumulate light into the light accumulation attachment, reading the g-buffer values directly from the other attachments. A full screen quad combines the light accumulation buffer with the albedo texture, samples the shadow map as a texture, and applies the light contribution from the sun.  The results of this composition pass overwrites the albedo attachment with the final composited output.  Lastly, particles representing each point light (lovingly called fairies) are rendered on top.  What began as the albedo texture in the g-buffer now contains the final value, which can be presented to the display as its texture is the CAMetalLayer's drawable texture.</sup> |
| [<sup>MetalImageProcessing</sup>](https://developer.apple.com/library/ios/samplecode/MetalImageProcessing/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>Metal</sup> | <sup>This sample extends the textured quad sample by adding a Metal compute encoder to convert the image to greyscale. Note the compute encoder is part of the same pass as the render encoder and hence demonstrates how you can use the same shared CPU/GPU data across compute and rendering.</sup> |
| [<sup>MetalInstancedHelix</sup>](https://developer.apple.com/library/ios/samplecode/MetalInstancedHelix/Introduction/Intro.html) | <sup></sup> | <sup>Metal</sup> | <sup>This example renders a set of cubes using Metal and alternates their colors by modifying each cube's uniforms directly in the shared CPU/GPU memory buffer. Several parameters can be modified directly in the AAPLRenderer.mm file including the number of cubes and their size. The cubes are rendered into a helix path using spherical coordinate system to get x,y,z for the translation matrix. Each cube is renderered individually using a basic 3D phong lighting shader, but drawn in only a single draw call using Metal's instancing API. Note, for each frame, each cube's transformation matrix is update along with its color, therefore in each frame the sample must traverese through 2n cubes.</sup> |
| [<sup>MetalShaderShowcase</sup>](https://developer.apple.com/library/ios/samplecode/MetalShaderShowcase/Introduction/Intro.html) | <sup></sup> | <sup>Metal</sup> | <sup>Metal Shader Showcase demonstrates a variety of visual techniques optimized with Metal. It contains 7 unique shaders: a Phong shader, a wood shader, a fog shader, a cel shader, a normal map shader, and a particle system shader. Using the Metal reflection API, the renderer queries the shaders for what arguments are needed and presents them to the render encoder.</sup> |
| [<sup>MetalTexturedQuad</sup>](https://developer.apple.com/library/ios/samplecode/MetalTexturedQuad/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>Metal</sup> | <sup>This sample shows how to create a basic textured quad in metal. It includes all transformations needed to ensure correct rendering orientation of the textured quad.</sup> |
| [<sup>MetalUniformStreaming</sup>](https://developer.apple.com/library/ios/samplecode/MetalUniformStreaming/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>Metal</sup> | <sup>Metal creates data buffer resources that can be read and written to on the CPU and GPU asynchronously. This example demonstrates using a data buffer to set uniforms for the vertex and fragment shaders.</sup> |
| [<sup>MetalVertexStreaming</sup>](https://developer.apple.com/library/ios/samplecode/MetalVertexStreaming/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>Metal</sup> | <sup>This sample shows how to stream vertex data between 3 command buffers using one block of memory shared by both the CPU and GPU. The original data is copied back into the Metal shared CPU/GPU buffer every frame and modified directly in the buffer to animate the triangle.</sup> |
| [<sup>MetalVideoCapture</sup>](https://developer.apple.com/library/ios/samplecode/MetalVideoCapture/Introduction/Intro.html) | <sup></sup> | <sup>Metal</sup> | <sup>This sample demonstrates how to stream captured video textures (from the front facing camera on an iOS device) into a 3D scene rendered with Metal. The video texture is combined with an environment map reflection from a cubemap (which is also rendered seperatly as the starfield skybox) and a 2D mipmap PVRTC texture (copper metal texture).</sup> |
| [<sup>Mixer iPodEQ AUGraph Test</sup>](https://developer.apple.com/library/ios/samplecode/iPhoneMixerEQGraphTest/Introduction/Intro.html) | <sup>Audio & Video</sup> | <sup>CoreAudio</sup> | <sup>Demonstrates how to build an Audio Unit Graph connecting a MultiChannel Mixer to the iPodEQ unit then to the RemoteIO unit.</sup> |
| [<sup>MotionEffects</sup>](https://developer.apple.com/library/ios/samplecode/sc1249/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>MotionEffects demonstrates applying motion effects to views in order to enhance the illusion of depth by creating parallxing effects.  This sample is broken down into three different parts, each demonstrating a unique way to use motion effects.</sup> |
| [<sup>MotionGraphs</sup>](https://developer.apple.com/library/ios/samplecode/MotionGraphs/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>CoreMotion</sup> | <sup>"MotionGraphs" demonstrates a how to use the push method to receive data from Core Motion. It displays graphs of accelerometer, gyroscope, and device motion data.</sup> |
| [<sup>MoveMe</sup>](https://developer.apple.com/library/ios/samplecode/MoveMe/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>This application illustrates simple drawing, touch handling, and animation using UIKit and Core Animation.</sup> |
| [<sup>MoviePlayer</sup>](https://developer.apple.com/library/ios/samplecode/MoviePlayer_iPhone/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Video)</sup> | <sup>MediaPlayer</sup> | <sup>Demonstrates how to use the Media Player framework to play a movie from a file or network stream, and configure the movie background color, playback controls, background color and image, scaling and repeat modes. It also shows how to draw custom overlay controls on top of the movie during playback.</sup> |
| [<sup>MTAudioProcessingTap Audio Processor</sup>](https://developer.apple.com/library/ios/samplecode/AudioTapProcessor/Introduction/Intro.html) | <sup>Audio & Video</sup> | <sup>AVFoundation</sup> | <sup>Sample application that uses the MTAudioProcessingTap in combination with AV Foundation to visualize audio samples as well as applying a Core Audio audio unit effect (Bandpass Filter) to the audio data.</sup> |
| [<sup>MultipeerGroupChat</sup>](https://developer.apple.com/library/ios/samplecode/MultipeerGroupChat/Introduction/Intro.html) | <sup>Networking & Internet</sup> | <sup>MultipeerConnectivity</sup> | <sup>Multipeer Group Chat is an example application which builds on the Multipeer Connectivity framework for discovering, connecting, and sharing data between "nearby" peers. This application uses framework based UI for connecting to local peers and the framework convenience API for exhanging text messages and image resources between devices.</sup> |
| [<sup>Multiple Selection with UITableView</sup>](https://developer.apple.com/library/ios/samplecode/TableMultiSelect/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Tables)</sup> | <sup>UIKit</sup> | <sup>"TableMultiSelect" demonstrates the use of multiple selection of table cells in UITableView, in particular using multiple selection to delete one or more items.</sup> |
| [<sup>MultipleDetailViews</sup>](https://developer.apple.com/library/ios/samplecode/MultipleDetailViews/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Windows & Views)</sup> | <sup>UIKit</sup> | <sup>This sample shows how you can use UISplitViewController to manage the presentation of multiple detail views in conjunction with a navigation hierarchy.</sup> |
| [<sup>MusicCube</sup>](https://developer.apple.com/library/ios/samplecode/MusicCube/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>GLKit</sup> | <sup>MusicCube demonstrates basic use of OpenGL ES, OpenAL, and Audio File Services on the iPhone for manipulating sound in a 3D environment. The four modes in the application illustrate how the sound volume and balance will change based on the position of the omnidirectional sound source and the position and rotation of the listener. Tap on the screen to switch between modes.</sup> |
| [<sup>MVCNetworking</sup>](https://developer.apple.com/library/ios/samplecode/MVCNetworking/Introduction/Intro.html) | <sup>Networking & Internet</sup> | <sup></sup> | <sup>MVCNetworking is a sample that shows how to create a network application using the Model-View-Controller design pattern.  Specifically, it displays a photo gallery by getting the gallery's XML description, thumbnails and photos from a web server, and uses Core Data to cache this information locally.</sup> |
| [<sup>MyImagePicker</sup>](https://developer.apple.com/library/ios/samplecode/MyImagePicker/Introduction/Intro.html) | <sup>General</sup> | <sup>AssetsLibrary</sup> | <sup>Demonstrates how to create an image picker user interface using the AssetsLibrary framework to display all albums, events and faces synced from iTunes.</sup> |
| [<sup>oalTouch</sup>](https://developer.apple.com/library/ios/samplecode/oalTouch/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>OpenAL</sup> | <sup>The code uses OpenAL to play a single audio source.  Move source or listener position by dragging icons around on the grid. Turn accelerometer functionality on to set listener orientation by tilting the device.</sup> |
| [<sup>Packaged Document for iOS</sup>](https://developer.apple.com/library/ios/samplecode/sc2281/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(File Management)</sup> | <sup>Foundation</sup> | <sup>"PackagedDocument_iOS" is a sample application for opening, editing and saving packaged documents using UIDocument and NSFileWrapper. This sample is ARC-enabled (Automatic Reference Counting).</sup> |
| [<sup>PageControl</sup>](https://developer.apple.com/library/ios/samplecode/PageControl/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>This application primarily demonstrates use of UIScrollView's paging functionality to use horizontal scrolling as a mechanism for navigating between different pages of content.  With the iPad, this type of user interface is not really necessary since the screen is larger allowing for more content and detailed information.</sup> |
| [<sup>pARk</sup>](https://developer.apple.com/library/ios/samplecode/pARk/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Event Handling)</sup> | <sup>CoreMotion</sup> | <sup>pARk is an application project that demonstrates a how to use Core Motion's true north-referenced attitude API. It contains a UIView subclass, ARView, that displays a live camera feed with places-of-interest overlaid at the appropriate coordinates. The places-of-interest used are some famous parks around the world.</sup> |
| [<sup>PhotoHandoff: Implementing NSUserActivity to hand off user actions</sup>](https://developer.apple.com/library/ios/samplecode/PhotoHandoff/Introduction/Intro.html) | <sup></sup> | <sup>UIKit</sup> | <sup>Demonstrates how to use NSUserActivity, based on “CollectionView-Simple” sample.  It is a universal sample that runs native both on iPhone and iPad.<br/><br/>- Enable Handoff for both devices (Settings -> General -> Enable Handoff)<br/><br/>- Both devices must be logged into the same iCloud account<br/><br/>- Bluetooth must be turned on (Settings -> Bluetooth)</sup> |
| [<sup>PhotoMap</sup>](https://developer.apple.com/library/ios/samplecode/PhotoMap/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>MapKit</sup> | <sup>The PhotoMap sample demonstrates how to load and display geotagged photos as Map Kit annotations. It further demonstrates how to cluster multiple annotations together to reduce on-screen clutter.</sup> |
| [<sup>PhotosByLocation</sup>](https://developer.apple.com/library/ios/samplecode/PhotosByLocation/Introduction/Intro.html) | <sup>General</sup> | <sup>AssetsLibrary</sup> | <sup>Demonstrates how to use the AssetsLibrary APIs to provide a custom image picking UI. The user experience is centered around the idea of using the assets location and time metadata as a basis for certain features.</sup> |
| [<sup>PhotoScroller</sup>](https://developer.apple.com/library/ios/samplecode/PhotoScroller/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>"PhotoScroller" demonstrates the use of embedded UIScrollViews and CATiledLayer to create a rich user experience for displaying and paginating photos that can be individually panned and zoomed. CATiledLayer is used to increase the performance of paging, panning, and zooming with high-resolution images or large sets of photos.</sup> |
| [<sup>PocketCoreImage</sup>](https://developer.apple.com/library/ios/samplecode/PocketCoreImage/Introduction/Intro.html) | <sup>Graphics & Animation</sup> | <sup>CoreImage</sup> | <sup>This sample demonstrates applying Core Image filters to a still image.  The filter configuration is done automatically (using random numbers) and multiple filters may be applied at the same time.  While this sample uses a preset list of filters that the user may select from, code is provided in the next section which demonstrates asking the system for a list of filters.</sup> |
| [<sup>Popover Controllers in iOS</sup>](https://developer.apple.com/library/ios/samplecode/Popovers/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Windows & Views)</sup> | <sup>UIKit</sup> | <sup>"Popovers" shows how to use UIPopoverController in iOS, including presentation, dismissal, and rotation of popovers. The sample uses a UISplitViewController to show how to present popovers from bar button items. It also demonstrates how you can ensure that multiple UIPopoverControllers are never presented at the same time.</sup> |
| [<sup>PrefsInCloud</sup>](https://developer.apple.com/library/ios/samplecode/PrefsInCloud/Introduction/Intro.html) | <sup>Data Management</sup> | <sup></sup> | <sup>A simple iOS iPhone application that demonstrates how to use iCloud key-value store to share a single piece of data, its background color, with the same app on other iOS devices.  It uses NSUbiquitousKeyValueStore to achieve this by storing a simple NSInteger representing a chosen color index.</sup> |
| [<sup>PrintPhoto</sup>](https://developer.apple.com/library/ios/samplecode/PrintPhoto/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(2D Drawing)</sup> | <sup>UIKit</sup> | <sup>PrintPhoto demonstrates how to print photos. The application allows a user to view and print any photo from the user's photo library. It initially presents a photo that is built into the application's bundle but by touching the photo picker icon you can choose any photo in the library.</sup> |
| [<sup>PVRTextureLoader</sup>](https://developer.apple.com/library/ios/samplecode/PVRTextureLoader/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(3D Drawing)</sup> | <sup>OpenGLES</sup> | <sup>This application illustrates how to load PVR texture files using the included PVRTexture class and then display them using OpenGL.</sup> |
| [<sup>Quartz Composer Conceptual Compositions</sup>](https://developer.apple.com/library/ios/samplecode/Conceptual/Introduction/Intro.html) | <sup>Graphics & Animation</sup> | <sup>QuartzCore</sup> | <sup>Compositions that demonstrate how to implement composition flow, image flow, music visualizer and other effects using Quartz Composer.</sup> |
| [<sup>Quartz Composer HistogramOperation</sup>](https://developer.apple.com/library/ios/samplecode/HistogramOperation/Introduction/Intro.html) | <sup>Graphics & Animation</sup> | <sup></sup> | <sup>A Quartz Composer plug-in that alters a source image according to the histogram of another image.</sup> |
| [<sup>Quartz Composer IMStatus</sup>](https://developer.apple.com/library/ios/samplecode/IMStatus/Introduction/Intro.html) | <sup>Graphics & Animation</sup> | <sup></sup> | <sup>A Quartz Composer plug-in that returns information about the logged in user and his or her buddies on a given instant messaging service.</sup> |
| [<sup>Quartz Composer iPatch</sup>](https://developer.apple.com/library/ios/samplecode/iPatch/Introduction/Intro.html) | <sup>Graphics & Animation</sup> | <sup></sup> | <sup>A Quartz Composer plug-in that converts any name to an "iName".</sup> |
| [<sup>Quartz Composer SQLiteQuery</sup>](https://developer.apple.com/library/ios/samplecode/SQLiteQuery/Introduction/Intro.html) | <sup>Graphics & Animation</sup> | <sup></sup> | <sup>A Quartz Composer plug-in that performs a query on a local SQlite database.</sup> |
| [<sup>Quartz2D for iOS</sup>](https://developer.apple.com/library/ios/samplecode/QuartzDemo/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(2D Drawing)</sup> | <sup>CoreGraphics</sup> | <sup>QuartzDemo is an iOS application that demonstrates many of the Quartz2D APIs made available by the CoreGraphics framework. Quartz2D forms the foundation of all drawing on iPhone OS and provides facilities for drawing lines, polygons, curves, images, gradients, PDF and many other graphical facilities.</sup> |
| [<sup>QuickContacts</sup>](https://developer.apple.com/library/ios/samplecode/QuickContacts/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Contact Data)</sup> | <sup>AddressBook</sup> | <sup>QuickContacts demonstrates how to use the Address Book UI controllers and various properties such as displayedProperties, allowsAddingToAddressBook, and displayPerson. It shows how to browse a list of Address Book contacts, display and edit a contact record, create a new contact record, and update a partial contact record.</sup> |
| [<sup>Reachability</sup>](https://developer.apple.com/library/ios/samplecode/Reachability/Introduction/Intro.html) | <sup>Networking & Internet</sup><br/><sup>(Services & Discovery)</sup> | <sup>SystemConfiguration</sup> | <sup>The Reachability sample application demonstrates how to use the SystemConfiguration framework to monitor the network state of an iOS device. In particular, it demonstrates how to know when IP can be routed and when traffic will be routed through a Wireless Wide Area Network (WWAN) interface such as EDGE or 3G.</sup> |
| [<sup>Real-time Video Processing Using AVPlayerItemVideoOutput</sup>](https://developer.apple.com/library/ios/samplecode/AVBasicVideoOutput/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Video)</sup> | <sup>AVFoundation</sup> | <sup>AVBasicVideoOutput demonstrates how to perform real-time video processing using AVPlayerItemVideoOutput and how to display processed video frames on screen using CAEAGLLayer and CADisplayLink. AVPlayerItemVideoOutput provides sample buffers (CVPixelBufferRef) which are then adjusted for their luma (Y) and chroma (UV) values based on the input from a user via UISliders. These processed pixel buffers are then rendered to a CAEAGLLayer.</sup> |
| [<sup>Reflection</sup>](https://developer.apple.com/library/ios/samplecode/Reflection/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(2D Drawing)</sup> | <sup>UIKit</sup> | <sup>This sample shows how to implement a "reflection" special effect on a given UIImageView most commonly seen in iTunes and iPod player apps.</sup> |
| [<sup>Regions</sup>](https://developer.apple.com/library/ios/samplecode/Regions/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>MapKit</sup> | <sup>This sample demonstrates proper use of region monitoring, significant location changes, and handling location events in the background on iOS. The sample uses an MKMapView that allows the user to add and remove regions to monitor, as well as a UITableView to display the region enter/exit/fail events that occur. When the application goes into the background, location updates are stopped and significant location changes are started. Likewise, when the application enters the foreground, location updates are started again and significant location changes are stopped. When location updates occur in the background, a badge is added to the homescreen icon displaying the number of region enter/exit/fail events logged.</sup> |
| [<sup>RosyWriter</sup>](https://developer.apple.com/library/ios/samplecode/RosyWriter/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Video)</sup> | <sup>AVFoundation</sup> | <sup>RosyWriter</sup> |
| [<sup>Sample Photo Editing Extension</sup>](https://developer.apple.com/library/ios/samplecode/SamplePhotoEditingExtension/Introduction/Intro.html) | <sup>Languages & Utilities</sup><br/><sup>(Graphics Tools)</sup> | <sup>UIKit</sup> | <sup>This sample shows how to implement a Photo Editing extension. This extension allows the user to select a filter effect to apply to the photo or video selected in Photos or Camera.</sup> |
| [<sup>Sample Print Page Renderer</sup>](https://developer.apple.com/library/ios/samplecode/Recipes_+_Printing/Introduction/Intro.html) | <sup>Graphics & Animation</sup> | <sup>UIKit</sup> | <sup>This sample demonstrates taking full control over the printed page using a UIPrintPageRenderer subclass. In addition to drawing full-page custom content, custom headers, and footers, it also shows how UIPrintFormatters can be utilized to do some heavy lifting.</sup> |
| [<sup>Sampler Unit Presets (LoadPresetDemo)</sup>](https://developer.apple.com/library/ios/samplecode/LoadPresetDemo/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AudioUnit</sup> | <sup>This sample code project shows how to create an an iOS audio processing graph containing a Sampler audio unit and how to configure the sampler by loading an AUPreset file that was created in Mac OS X. The project also shows how to start the graph and trigger note-on and note-off events to audition the presets.</sup> |
| [<sup>SceneKit State of the Union Demo</sup>](https://developer.apple.com/library/ios/samplecode/SceneKitReel/Introduction/Intro.html) | <sup></sup> | <sup>SceneKit</sup> | <sup>This sample shows how the demo for the state the of union was realized. It includes examples of physics simulation, particles, collisions, physics field, 3D text, the integration with SpriteKit and custom GLSL shaders.</sup> |
| [<sup>SceneKit Vehicle Demo</sup>](https://developer.apple.com/library/ios/samplecode/SceneKitVehicle/Introduction/Intro.html) | <sup></sup> | <sup>SceneKit</sup> | <sup>This sample code shows how to simulate a vehicle using the SCNPhysicsVehicle behaviour. The vehicle can be controller with either the accelerometer or a game controller. It also illustrate basic physics interaction and game overlays done with SpriteKit.</sup> |
| [<sup>Simple Background Transfer</sup>](https://developer.apple.com/library/ios/samplecode/SimpleBackgroundTransfer/Introduction/Intro.html) | <sup>Networking & Internet</sup> | <sup>UIKit</sup> | <sup>"SimpleBackgroundTransfer" illustrates how to support Background Transfer using NSURLSession to manage a download task.</sup> |
| [<sup>Simple Core Data Relationships</sup>](https://developer.apple.com/library/ios/samplecode/TaggedLocations/Introduction/Intro.html) | <sup>Data Management</sup> | <sup>CoreData</sup> | <sup>TaggedLocations illustrates how you can change Core Data attributes and relationships in an iOS application.</sup> |
| [<sup>Simple Gesture Recognizers</sup>](https://developer.apple.com/library/ios/samplecode/SimpleGestureRecognizers/Introduction/Intro.html) | <sup>Data Management</sup><br/><sup>(Event Handling)</sup> | <sup>UIKit</sup> | <sup>This sample shows how you can easily use UITapGestureRecognizer, UISwipeGestureRecognizer, and UIRotationGestureRecognizer to detect user gestures. It illustrates the use of basic target-action messaging using recognizers, and also shows: how you can toggle the behavior of a recognizer at runtime using delegation; options for maintaining references to recognizers; and interaction with the exclusive touch setting for views.</sup> |
| [<sup>Simple UISearchBar with State Restoration</sup>](https://developer.apple.com/library/ios/samplecode/TableSearch/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Tables)</sup> | <sup>UIKit</sup> | <sup>"TableSearch" demonstrates how to use a UISearchDisplayController object. When you present a large amount of data in a table view, you can use a search display controller to let the user search the contents of the table view to quickly find items of interest. The sample shows how you can display data in a table view and use a search display controller to display a search bar and another table view that shows the results of a search. The sample also illustrates use of state restoration.</sup> |
| [<sup>SimpleEKDemo</sup>](https://developer.apple.com/library/ios/samplecode/SimpleEKDemo/Introduction/Intro.html) | <sup></sup> | <sup>EventKit</sup> | <sup>The application demonstrates how to check and request access to a user's Calendar. It uses table views to display EKCalendar object and EKEvent objects retrieved from an EKEventStore object. It implements EKEventViewController for viewing and editing existing EKEvents, and uses EKEventEditViewController for creating new EKEvents.</sup> |
| [<sup>SimpleFTPSample</sup>](https://developer.apple.com/library/ios/samplecode/SimpleFTPSample/Introduction/Intro.html) | <sup>Networking & Internet</sup><br/><sup>(Protocol Streams)</sup> | <sup>Foundation</sup> | <sup>SimpleFTPSample shows how to do simple FTP operations using the NSURLConnection and CFFTPStream APIs.  It can download a file using both NSURLConnection and CFFTPStream.  Also, it can upload a file, list a directory, and create a directory using CFFTPStream.</sup> |
| [<sup>SimpleNetworkStreams</sup>](https://developer.apple.com/library/ios/samplecode/SimpleNetworkStreams/Introduction/Intro.html) | <sup>Networking & Internet</sup><br/><sup>(Sockets & TCP)</sup> | <sup></sup> | <sup>Shows how to do simple networking using the NSStream API.  The goal of this sample is very limited: it does not demonstrate everything you need to implement a fully fledged networking product, rather, it focuses on using the NSStream API to move a realistic amount of data across the network.</sup> |
| [<sup>SimpleUndo</sup>](https://developer.apple.com/library/ios/samplecode/SimpleUndo/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>Foundation</sup> | <sup>The root view controller displays information (title, author, and copyright date) about a book. The user can edit this information by tapping Edit in the navigation bar. When editing starts, the root view controller creates an undo manager to record changes. The undo manager supports up to three levels of undo and redo.  When the user taps Done, changes are considered to be committed and the undo manager is disposed of.</sup> |
| [<sup>SimpleURLConnections</sup>](https://developer.apple.com/library/ios/samplecode/SimpleURLConnections/Introduction/Intro.html) | <sup>Networking & Internet</sup><br/><sup>(Protocol Streams)</sup> | <sup>Foundation</sup> | <sup>SimpleURLConnections shows how to do simple networking using the NSURLConnection API.  The goal of this sample is very limited: it does not demonstrate everything you need to implement a fully fledged networking product, rather, its goal is to demonstrate simple HTTP GET, PUT and POST using the NSURLConnection API.</sup> |
| [<sup>SpeakHere</sup>](https://developer.apple.com/library/ios/samplecode/SpeakHere/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Audio)</sup> | <sup>AudioToolbox</sup> | <sup>SpeakHere demonstrates basic use of Audio Queue Services, Audio File Services, and Audio Session Services on the iPhone for recording and playing back audio.</sup> |
| [<sup>SquareCam</sup>](https://developer.apple.com/library/ios/samplecode/SquareCam/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Video)</sup> | <sup>AVFoundation</sup> | <sup>SquareCam demonstrates improvements to the AVCaptureStillImageOutput class in iOS 5, highlighting the following features:</sup> |
| [<sup>State Restoration</sup>](https://developer.apple.com/library/ios/samplecode/StateRestore/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>Demonstrates how to implement and debug the APIs for "State Preservation and Restoration".</sup> |
| [<sup>State Restoration of Child View Controllers</sup>](https://developer.apple.com/library/ios/samplecode/StateRestoreChildViews/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>Demonstrates how to implement "State Preservation and Restoration" in an app with child view controllers.  The sample contains one parent view controller, which can host two different child view controllers. The user taps the segmented control to toggle between the two different children.</sup> |
| [<sup>StitchedStreamPlayer</sup>](https://developer.apple.com/library/ios/samplecode/StitchedStreamPlayer/Introduction/Intro.html) | <sup>Audio & Video</sup> | <sup>AVFoundation</sup> | <sup>A simple AVFoundation demonstration of how timed metadata can be used to identify different content in a stream, supporting a custom seek UI.</sup> |
| [<sup>StopNGo for iOS</sup>](https://developer.apple.com/library/ios/samplecode/StopNGo/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Video)</sup> | <sup>AVFoundation</sup> | <sup>StopNGo is a simple stop-motion animation QuickTime movie recorder that uses AVFoundation.</sup> |
| [<sup>StoreKitSuite</sup>](https://developer.apple.com/library/ios/samplecode/sc1991/Introduction/Intro.html) | <sup></sup> | <sup>StoreKit</sup> | <sup>StoreKitSuite consists of the iOSInAppPurchases and IAPStoreProductViewController sample codes that demonstrate how to implement In-App Purchase.</sup> |
| [<sup>StreetScroller</sup>](https://developer.apple.com/library/ios/samplecode/StreetScroller/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Controls)</sup> | <sup>UIKit</sup> | <sup>Demonstrates how a UIScrollView subclass can scroll infinitely in the horizontal direction.</sup> |
| [<sup>Table Search with UISearchController (Obj-C and Swift)</sup>](https://developer.apple.com/library/ios/samplecode/TableSearch_UISearchController/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>“Table Search with UISearchController” is an iOS sample application that demonstrates how to use UISearchController. A search controller manages the presentation of a search bar (in concert with the results view controller’s content). The Xcode project includes two separate projects to support the following languages: Objective-C and Swift.</sup> |
| [<sup>Table View Animations and Gestures</sup>](https://developer.apple.com/library/ios/samplecode/TableViewUpdates/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Tables)</sup> | <sup>UIKit</sup> | <sup>"TableViewUpdates" demonstrates how you can use animated updates to open and close sections of a table view for viewing, where each section represents a play, and each row contains a quotation from the play. It also uses gesture recognizers to respond to user input: * A UITapGestureRecognizer to allow tapping on the section headers to expand the section; * A UIPinchGestureRecognizer to allow dynamic changes to the height of table view rows; and * A UILongPressGestureRecognizer to allow press-and-hold on table view cells to initiate an email of the quotation.</sup> |
| [<sup>TableView Fundamentals for iOS</sup>](https://developer.apple.com/library/ios/samplecode/TableViewSuite/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Tables)</sup> | <sup>UIKit</sup> | <sup>This sample shows how to use UITableView and UITableViewController through a progression of increasingly advanced applications that display information about time zones.</sup> |
| [<sup>TableViewCell Accessory</sup>](https://developer.apple.com/library/ios/samplecode/Accessory/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Tables)</sup> | <sup>UIKit</sup> | <sup>This sample demonstrates two methods that can be used to implement a custom accessory view in your UITableViewCell's.  In both examples, a custom control that implements a toggle-able checkbox is used.</sup> |
| [<sup>Tabster</sup>](https://developer.apple.com/library/ios/samplecode/Tabster/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Windows & Views)</sup> | <sup>UIKit</sup> | <sup>An eclectic-style application designed to show how to build a tab-bar based iOS application.</sup> |
| [<sup>Teslameter</sup>](https://developer.apple.com/library/ios/samplecode/Teslameter/Introduction/Intro.html) | <sup></sup> | <sup>CoreLocation</sup> | <sup>This application implements a Teslameter, a magnetic field detector. It displays the raw x, y, and z magnetometer values, a plotted history of those values, and a computed magnitude (size or strength) of the magnetic field.</sup> |
| [<sup>TheElements</sup>](https://developer.apple.com/library/ios/samplecode/TheElements/Introduction/Intro.html) | <sup>Data Management</sup> | <sup>UIKit</sup> | <sup>TheElements is a sample application that provides access to the data contained in the Periodic Table of the Elements. The Periodic Table of the Elements catalogs all the known atomic elements in the universe.</sup> |
| [<sup>TopSongs</sup>](https://developer.apple.com/library/ios/samplecode/TopSongs/Introduction/Intro.html) | <sup>Performance</sup> | <sup>CoreData</sup> | <sup>This sample code demonstrates efficiently parsing and importing data from an XML RSS feed into Core Data. The feed is from iTunes Top Songs and contains data about songs, artists, and categories. The application's data model has an entity for Song and for Category. Managed objects are inserted into a managed object context on a background thread, so the application remains responsive to the user while the import is taking place.</sup> |
| [<sup>UICatalog: Creating and Customizing UIKit Controls (Obj-C and Swift)</sup>](https://developer.apple.com/library/ios/samplecode/UICatalog/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>This sample demonstrates how to use many views and controls in the UIKit framework along with their assorted functionalities. Refer to this sample if you are looking for specific controls or views that are provided by the system. This sample also shows you how to make your non-standard views (images or custom views) accessible. Using the iOS Accessibility API enhances the user experience of VoiceOver users.</sup> |
| [<sup>UIImagePicker Video Recorder</sup>](https://developer.apple.com/library/ios/samplecode/VideoRecorder/Introduction/Intro.html) | <sup>Audio & Video</sup><br/><sup>(Video)</sup> | <sup>UIKit</sup> | <sup>Demonstrates how to create a custom UI for the camera variant of the UIImagePickerController and how to programmatically control video recording.</sup> |
| [<sup>UIKit Dynamics Catalog</sup>](https://developer.apple.com/library/ios/samplecode/DynamicsCatalog/Introduction/Intro.html) | <sup>Graphics & Animation</sup> | <sup>UIKit</sup> | <sup>UIKit Dynamics Catalog illustrates a number of uses of UIKit Dynamics, the iOS API that provides physics-related capabilities and animations to views and other dynamic items.  Each of the 10 view controllers in this project shows a different way to use UIKit Dynamics—-in many cases, combining dynamic behaviors for interesting results.</sup> |
| [<sup>UIKit Printing with UIPrintInteractionController and UIViewPrintFormatter</sup>](https://developer.apple.com/library/ios/samplecode/PrintWebView/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(2D Drawing)</sup> | <sup>UIKit</sup> | <sup>PrintWebView demonstrates how to print the content displayed by a UIWebView object using the UIViewPrintFormatter class. This sample application is a primitive web browser with printing capability.</sup> |
| [<sup>Unit Testing Apps and Frameworks</sup>](https://developer.apple.com/library/ios/samplecode/UnitTests/Introduction/Intro.html) | <sup>Xcode</sup><br/><sup>(IDEs)</sup> | <sup></sup> | <sup>Shows how to build a static library for an iOS app and a Mac app, how to implement and run logic unit tests on the library source code on each platform, and how to implement and run application unit tests for the apps.</sup> |
| [<sup>UnwindSegue</sup>](https://developer.apple.com/library/ios/samplecode/UnwindSegue/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>This sample demonstrates how to use segues and unwind segues to drive navigation between the various screens of content in your app.  The project contains two build targets: UnwindSegue and CustomUnwindSegue.  UnwindSegue demonstrates using unwind segues with modally presented view controllers as well as view controllers in a UINavigationController.  CustomUnwindSegue demonstrates implementing a custom container view controller that can be used with unwind segues.</sup> |
| [<sup>URLCache</sup>](https://developer.apple.com/library/ios/samplecode/URLCache/Introduction/Intro.html) | <sup>Performance</sup> | <sup></sup> | <sup>URLCache is a sample iPhone application that demonstrates how to download a resource off the web, store it in the application's data directory, and use the local copy of the resource. URLCache also demonstrates how to implement a couple of caching policies:</sup> |
| [<sup>Using a Search Bar in a Toolbar</sup>](https://developer.apple.com/library/ios/samplecode/ToolbarSearch/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Controls)</sup> | <sup>UIKit</sup> | <sup>"ToolbarSearch" shows how to use a search bar in a toolbar and present recent searches in a popover.</sup> |
| [<sup>Using AirPrint to Print a Banner</sup>](https://developer.apple.com/library/ios/samplecode/PrintBanner/Introduction/Intro.html) | <sup>General</sup> | <sup></sup> | <sup>PrintBanner shows how to print a banner of arbitrary length to a roll printer. The code formats the job for landscape orientation and then calculates a font size that fills the width of the paper. It then determines the length of the text and pads the length so the final cut length results in a pleasing layout.</sup> |
| [<sup>Using External Displays</sup>](https://developer.apple.com/library/ios/samplecode/ExternalDisplay/Introduction/Intro.html) | <sup>User Experience</sup><br/><sup>(Windows & Views)</sup> | <sup>UIKit</sup> | <sup>How to detect the presence of an external display, determine the available display resolutions, select a resolution, and show content on the display.</sup> |
| [<sup>Using NSXMLParser to parse XML documents</sup>](https://developer.apple.com/library/ios/samplecode/SeismicXML/Introduction/Intro.html) | <sup>Data Management</sup> | <sup>UIKit</sup> | <sup>"SeismicXML" demonstrates how to use NSXMLParser to parse XML data. The XML parsing occurs on a background thread using NSOperation and updates the earthquakes table view with batches of parsed objects.</sup> |
| [<sup>Using UIImagePickerController to Select Pictures and Take Photos</sup>](https://developer.apple.com/library/ios/samplecode/PhotoPicker/Introduction/Intro.html) | <sup>User Experience</sup> | <sup>UIKit</sup> | <sup>PhotoPicker demonstrates how to choose images from the photo library, take a picture with the device's camera, and how to customize the look of the camera's user interface.  This is done by using UIImagePickerController.  The chosen image or camera photo is displayed in a UIImageView.  To customize the camera's interface, this sample shows how to use an overlay view.  With this overlay view it gives you the ability to customize the UI as you take a picture.</sup> |
| [<sup>VideoSnake</sup>](https://developer.apple.com/library/ios/samplecode/VideoSnake/Introduction/Intro.html) | <sup>Audio & Video</sup> | <sup>AVFoundation</sup> | <sup>VideoSnake demonstrates temporal synchronization of video with motion data.</sup> |
| [<sup>ViewTransitions</sup>](https://developer.apple.com/library/ios/samplecode/ViewTransitions/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(Animation)</sup> | <sup>UIKit</sup> | <sup>Demonstrates how to perform transitions between two views using UIView's transitionFromView and animateWithDuration API.</sup> |
| [<sup>WiTap</sup>](https://developer.apple.com/library/ios/samplecode/WiTap/Introduction/Intro.html) | <sup>Networking & Internet</sup><br/><sup>(Services & Discovery)</sup> | <sup>Foundation</sup> | <sup>The WiTap sample demonstrates peer-to-peer networking over Wi-Fi and Bluetooth. Using Bonjour, the application both advertises itself on the local network and displays a list of other instances on the network. Supports infrastructure networks, peer-to-peer Bluetooth, and peer-to-peer Wi-Fi (on compatible hardware).</sup> |
| [<sup>XMLPerformance</sup>](https://developer.apple.com/library/ios/samplecode/XMLPerformance/Introduction/Intro.html) | <sup>Performance</sup> | <sup>Foundation</sup> | <sup>This sample explores two approaches to parsing XML, focusing on performance with respect to speed, memory footprint, and user experience. The XML data used is the current "Top 300" songs from the iTunes store. The data itself is not particularly important to the sample - it was chosen because of its simplicity, availability, and because the size (approximately 850KB) is sufficient to demonstrate the performance issues central to the sample.</sup> |
| [<sup>ZoomingPDFViewer</sup>](https://developer.apple.com/library/ios/samplecode/ZoomingPDFViewer/Introduction/Intro.html) | <sup>Graphics & Animation</sup><br/><sup>(2D Drawing)</sup> | <sup>CoreGraphics</sup> | <sup>Multi-paged PDF viewing with UIPageViewController demonstrates two-page spline viewing in landscape orientation, which looks like a book within iBooks. The sample also uses UIScrollView and CATiledLayer to support zooming within a single-page view used in portrait orientations. This app is universal and only supports the two-page spline view in landscape orientation on iPad.</sup> |